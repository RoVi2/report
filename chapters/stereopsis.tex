%!TEX root = ../report.tex

\chapter{Stereovision} % (fold)
\label{chap:stereopsis}

\section{Introduction}
In this section, the solution adopted for the location of the target in the 3D space through the vision system, the reasons that led to this approach, the assumptions made and the obtained results are presented. 
A more detailed explanation of the 2D image processing performed for the target tracking can be found in \ref{chap:feature_extraction}.

\section{Stereopsis}
Due to the fact that two vision systems were available on the setup, the first step was to decide which one to employ and the method to compute disparity. 
After a slight evaluation of resources the chosen one was the stereo rig, utilized to carry out sparse stereo with a single well-defined point. 
Thus, the next points contain the steps to compute the position of a point in 3-space given its image in two views and the camera matrices. 

Through the acquired pairs of images, the pixel coordinates of the center of gravity of the target are extracted by the algorithm presented in \ref{chap:feature_extraction}, this has not been the approach adopted here.  and they are used to calculate its 3D coordinates referred to the camera reference frame by means of linear triangulation methods.

\section{Camera model}
As a consequence of the need of a defined relation between the image and the object space, the cameras must be used as a mapping. 
The mathematical model assumed for the camera is the basic pinhole structure, described in \cite{Hartley}. 
This model defines a linear projection from the 3D world to the image plane, and contains the intrinsic parameters that relates the camera and the image reference frames as showed in equation \ref{eq:pinhole_model}. Where X represent the homogeneous coordinates of a 3D point and x its projection in the image plane in pixel coordinates.
 
\begin{equation}
x = P·X
\label{eq:pinhole_model}
\end{equation}  

Since the linearity assumed by this model does not hold due to imperfections in the lens, a distortion model needs to be added to the calculations in order to correct the deviations on the real cameras. 
The Plumb Bob distortion model, introduced in \cite{Brown} is applied here since it is the one used by ROS.

\subsection{Calibration}
In order to obtain the intrinsic parameters for the camera model, a stereo calibration process based on the technique presented in \cite{Zhang} has been carried out for both Bumblebee2 1394a cameras by means of a ROS node. 

The node only needs to be provided enough images of a 2D calibration pattern in different positions. 
It computes the projection of the points on the pattern and solves the resulting equations system in a Least-squares fashion as described in \cite{Hartley}. 
The output of the node are the camera and projection matrices of the cameras, the set of parameters for the distortion model and the homography for the image rectification. 
Since the projection matrices will be used for the computation of the fundamental matrix during the triangulation calculus, their quality will heavily influence on the overall performance of the system. 
Therefore, a special commitment has been applied to this step. 

\section{3D Reconstruction}
In order to compute the depth for a pair of correspondent image points, a suitable triangulation method light but robust enough to be executed on line with sufficient accuracy must be found. 
The task, trivial in theory, entails certain complexity due to the acquisition errors in the measured image coordinates. 
An immediate, simple approach could consists in back-projecting the correspondent image points, by means of the projection matrices, and finding the 3D point in the space in which they intersect. 
However, this procedure is very unlikely to succeed in most of the cases since the image points do not satisfy the epipolar constraint due the errors.

\begin{figure}[h]
    \centering
   \includegraphics[width=0.8\textwidth]{figures/back_projection}
    \caption{Back-Projection rays error}
    \label{fig:Back-Projection}
\end{figure}

This is because in most of the cases the rays will not intersect in the 3D point, since there is not a point X that satisfies the equations \ref{eq:projection}. 

\begin{equation}
	\centering
	x  = P·X  $$\\
	x' = P'X $$
	\label{eq:projection}
\end{equation} 

\subsection{Triangulation}
The applied method to estimate depth and compute the estimations of 3D positions is the defined for the OpenCV functions utilized in the developed algorithm. 
This method is based on the direct linear transformation procedure to compute the estimation of the 3D point.

\subsubsection{Direct Linear Transformation}
The equations \ref{eq:projection} are combined into a set of linear equations in X of the form \ref{eq:DLT} by eliminating the scale factor, where the matrix A is \ref{eq:DLT2}

\begin{equation}
	\centering
	AX = 0 
	\label{eq:DLT}
\end{equation}

\begin{equation}
A =
 \begin{pmatrix}
  xP^{3T} - P^{1T} \\
  yP^{3T} - P^{2T} \\
  x'P^{'3T} - P^{'1T} \\
  yP^{'3T} - P^{'2T}
 \end{pmatrix}
 \label{eq:DLT2}
\end{equation}

The way the DTL algorithm solves this set of redundant equations in the unknowns of X is by means of a Least-squares minimization problem in the absence of an exact solution. 
The presence of noise reduces the problem to the minimization of \ref{eq:LSM} where e stands for the projection errors.
The LS solution solution is given in equation \ref{eq:LSM_solution}.

\begin{equation}
	\centering
	\lVert \mathbf{AX-e} \rVert 
	\label{eq:LSM}
\end{equation}

\begin{equation}
	\centering
	X = [A^{T}A]^{-1}A^{T}e
	\label{eq:LSM_solution}
\end{equation}

It is known that a minimization of the geometric error for a given correspondent pair of points \{x, xi\}. The way to do so would be to minimize the sum of squared distances to find a pair of points \{\^x, \^xi\} subject to the epipolar constraint. However, not enough documentation about OpenCV was found in order to find out if this process is implemented in the applied functions.





% section stereopsis (end)
  